{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Alles runnen totdat je CODE MEREL ziet, dan het blokje runnen met jouw naam!",
   "id": "4b1697b99591a86c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "from ema_workbench import (\n",
    "    Model,\n",
    "    Policy,\n",
    "    ema_logging,\n",
    "    SequentialEvaluator,\n",
    "    MultiprocessingEvaluator,\n",
    "    Samplers,\n",
    ")\n",
    "from dike_model_function import DikeNetwork  # @UnresolvedImport\n",
    "from problem_formulation import get_model_for_problem_formulation, sum_over, sum_over_time\n",
    "\n",
    "# Define the folder path and get list of files\n",
    "folder_name = \"selected_4_policies\"\n",
    "file_list = os.listdir(folder_name)\n",
    "\n",
    "# Initialize empty lists to hold dataframes for policies_20 and policies_21\n",
    "policies_20_dfs = []\n",
    "policies_21_dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(os.path.join(folder_name, file), nrows=1)\n",
    "\n",
    "        # Check if file name contains \"_20_\" or \"_21_\" and add sliced df accordingly\n",
    "        if '_20_' in file:\n",
    "            # Take rows 20 to 50 by index (i.e. iloc[19:50])\n",
    "            policies_20_dfs.append(df)\n",
    "        elif '_21_' in file:\n",
    "            policies_21_dfs.append(df)\n",
    "\n",
    "\n",
    "policies_20_df = pd.concat(policies_20_dfs, ignore_index=True).iloc[:, 19:50]\n",
    "policies_21_df = pd.concat(policies_21_dfs, ignore_index=True).iloc[:, 19:50]"
   ],
   "id": "459b93e82adf3fdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies_20 = []\n",
    "\n",
    "for idx, row in policies_20_df.iterrows():\n",
    "    # Convert the row (pandas Series) to a dictionary: lever_name -> lever_value\n",
    "    lever_values = row.to_dict()\n",
    "\n",
    "    # Create a unique policy name, e.g. \"policy_20_row_0\"\n",
    "    policy_name = f\"policy_20_row_{idx}\"\n",
    "\n",
    "    # Create the Policy object\n",
    "    policy = Policy(policy_name, **lever_values)\n",
    "\n",
    "    # Append to the list\n",
    "    policies_20.append(policy)\n",
    "\n",
    "policies_21 = []\n",
    "\n",
    "for idx, row in policies_21_df.iterrows():\n",
    "    # Convert the row (pandas Series) to a dictionary: lever_name -> lever_value\n",
    "    lever_values = row.to_dict()\n",
    "\n",
    "    # Create a unique policy name, e.g. \"policy_20_row_0\"\n",
    "    policy_name = f\"policy_21_row_{idx}\"\n",
    "\n",
    "    # Create the Policy object\n",
    "    policy = Policy(policy_name, **lever_values)\n",
    "\n",
    "    # Append to the list\n",
    "    policies_21.append(policy)\n",
    "\n"
   ],
   "id": "cf5a92859d7d25a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def save_experiments_and_outcomes(name, experiments, outcomes, folder_name=\"robustness_results_other_aggregation\"):\n",
    "    \"\"\"\n",
    "    Saves the experiments and outcomes to CSV files in the specified folder,\n",
    "    with filenames that include a custom name and timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - name (str): Custom label for the output files (e.g., 'policy6_seed20').\n",
    "    - experiments (pd.DataFrame): The experiments DataFrame.\n",
    "    - outcomes (dict): The outcomes dictionary from the EMA Workbench.\n",
    "    - folder_name (str): The name of the folder where files will be saved. Default is \"results\".\n",
    "    \"\"\"\n",
    "    # Get timestamp\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Define file paths\n",
    "    exp_path = os.path.join(folder_name, f'{name}_robustness_50000_experiments_{current_date}.csv')\n",
    "    out_path = os.path.join(folder_name, f'{name}_robustness_50000_outcomes_{current_date}.csv')\n",
    "\n",
    "    # Save experiments\n",
    "    experiments.to_csv(exp_path, index=False)\n",
    "\n",
    "    # Flatten outcomes and save\n",
    "    flattened_data = {key: value.flatten() for key, value in outcomes.items()}\n",
    "    df_outcomes = pd.DataFrame(flattened_data)\n",
    "    df_outcomes.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Saved experiments and outcomes as '{name}' in '{folder_name}' (timestamp: {current_date})\")\n"
   ],
   "id": "bc0b16baad848601"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CODE MEREL",
   "id": "370f5ef9e5213b65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 50000\n",
    "np.random.seed(21)\n",
    "policy = policies_20[0]\n",
    "\n",
    "# Load your model and define parameters\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(3)\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(\n",
    "        scenarios=n_scenarios,\n",
    "        policies=policy,\n",
    "        uncertainty_sampling=Samplers.LHS\n",
    "    )\n",
    "\n",
    "print(results)\n",
    "\n",
    "experiments, outcomes = results\n",
    "save_experiments_and_outcomes('policy_6_s20', experiments, outcomes)\n"
   ],
   "id": "abbe89c35a979cf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef31a7da1ed5f596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CODE TWAN",
   "id": "b6cb92fad27ed680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 50000\n",
    "np.random.seed(21)\n",
    "policy = policies_20[1]\n",
    "\n",
    "# Load your model and define parameters\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(3)\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(\n",
    "        scenarios=n_scenarios,\n",
    "        policies=policy,\n",
    "        uncertainty_sampling=Samplers.LHS\n",
    "    )\n",
    "\n",
    "experiments, outcomes = results\n",
    "save_experiments_and_outcomes('policy_8_s20', experiments, outcomes)\n"
   ],
   "id": "5ed0d90c3259916b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CODE JESSE",
   "id": "5285f6aead68ca7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 50000\n",
    "np.random.seed(21)\n",
    "policy = policies_21[0]\n",
    "\n",
    "# Load your model and define parameters\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(3)\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(\n",
    "        scenarios=n_scenarios,\n",
    "        policies=policy,\n",
    "        uncertainty_sampling=Samplers.LHS\n",
    "    )\n",
    "\n",
    "experiments, outcomes = results\n",
    "save_experiments_and_outcomes('policy_7_s21', experiments, outcomes)\n"
   ],
   "id": "c9de2464792a4eee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CODE PIEN",
   "id": "ba75de489ec0ffb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 50000\n",
    "np.random.seed(21)\n",
    "policy = policies_21[1]\n",
    "\n",
    "# Load your model and define parameters\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(3)\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(\n",
    "        scenarios=n_scenarios,\n",
    "        policies=policy,\n",
    "        uncertainty_sampling=Samplers.LHS\n",
    "    )\n",
    "\n",
    "experiments, outcomes = results\n",
    "save_experiments_and_outcomes('policy_8_s21', experiments, outcomes)\n"
   ],
   "id": "d379639b64b6b33c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "flattened_data = {key: value.flatten() for key, value in outcomes.items()}\n",
    "df_outcomes = pd.DataFrame(flattened_data)\n",
    "df_outcomes.columns"
   ],
   "id": "c4ea1c141853af7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8dff8abda03707b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "91f05ac05bee4194"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "900780d3de5f09ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19e82c2864cb2eb0"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
