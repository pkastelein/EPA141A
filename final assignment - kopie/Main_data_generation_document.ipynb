{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9827ea2",
   "metadata": {},
   "source": [
    "# Main Notebook for Data Generation\n",
    "\n",
    "This notebook is responsible for generating the data required for analysis and visualization in the results section of the project. The generated data is used in the `Main_document.ipynb` notebook to create figures and perform further analysis.\n",
    "\n",
    "### Key Points:\n",
    "- **Problem Formulation:** Problem formulation ID 2 is primarily used for data generation.\n",
    "- **Random Seeds:** Two main seeds (`20` and `21`) are used to ensure reproducibility. However, a seed of `42` was hardcoded in some parts of the code, which is noted in the comments.\n",
    "- **Language Model Assistance:** Some parts of the code and comments were generated with the help of large language models like ChatGPT.\n",
    "\n",
    "### Workflow:\n",
    "1. Import necessary libraries.\n",
    "2. Initialize the model using the specified problem formulation.\n",
    "3. Generate data for various sections, including exploration, sensitivity analysis, optimization, and robustness testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dadfefc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T22:59:18.450079Z",
     "start_time": "2025-06-19T22:59:14.457721Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "\n",
    "from ema_workbench import MultiprocessingEvaluator, Policy, Scenario, Samplers\n",
    "from ema_workbench.analysis import dimensional_stacking, parcoords, prim\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, EpsNSGAII\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from ema_workbench.util import ema_logging\n",
    "\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display all columns in pandas DataFrames\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Initialize model\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiments_and_outcomes(name, experiments, outcomes, folder_name=\"sobol_results\"):\n",
    "    \"\"\"\n",
    "    Saves the experiments and outcomes to CSV files in the specified folder,\n",
    "    with filenames that include a custom name and timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - name (str): Custom label for the output files (e.g., 'policy6_seed20').\n",
    "    - experiments (pd.DataFrame): The experiments DataFrame.\n",
    "    - outcomes (dict): The outcomes dictionary from the EMA Workbench.\n",
    "    - folder_name (str): The name of the folder where files will be saved. Default is \"results\".\n",
    "    \"\"\"\n",
    "    # Get timestamp\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Define file paths\n",
    "    exp_path = os.path.join(folder_name, f'{name}_experiments.csv')\n",
    "    out_path = os.path.join(folder_name, f'{name}_policies.csv')\n",
    "\n",
    "    # Save experiments\n",
    "    experiments.to_csv(exp_path, index=False)\n",
    "\n",
    "    # Flatten outcomes and save\n",
    "    flattened_data = {key: value.flatten() for key, value in outcomes.items()}\n",
    "    df_outcomes = pd.DataFrame(flattened_data)\n",
    "    df_outcomes.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Saved experiments and outcomes as '{name}' in '{folder_name}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8f826",
   "metadata": {},
   "source": [
    "## Open Exploration\n",
    "\n",
    "### Zero Policy\n",
    "\n",
    "This section generates data for the 'Zero Policy' subsection. The zero policy represents a scenario where no interventions are applied (all levers are set to zero). The data generated here is used to analyze the baseline outcomes without any policy interventions.\n",
    "\n",
    "### Workflow:\n",
    "1. Define the number of scenarios to simulate (`n_scenarios = 100000`).\n",
    "2. Create a policy dictionary where all levers are set to zero.\n",
    "3. Use the `MultiprocessingEvaluator` to perform experiments with the zero policy across the defined scenarios.\n",
    "4. Save the generated experiments and outcomes to CSV files for further analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c7d9f28cc348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = 100000\n",
    "\n",
    "def get_do_nothing_dict():\n",
    "    return {l.name: 0 for l in dike_model.levers}\n",
    "\n",
    "policies = [\n",
    "    Policy(\n",
    "        \"policy 0\",\n",
    "        **dict(\n",
    "            get_do_nothing_dict()\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "np.random.seed(20)\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios, policies=policies, uncertainty_sampling=Samplers.LHS)\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "folder_name = \"zero_policy_results\"\n",
    "experiments.to_csv(os.path.join(folder_name, f'scenario_space_100000_experiments.csv'), index=False)\n",
    "\n",
    "# Save outcomes DataFrame to a CSV file in the folder\n",
    "flattened_data = {key: value.flatten() for key, value in outcomes.items()}\n",
    "df_outcomes = pd.DataFrame(flattened_data)\n",
    "\n",
    "#df_outcomes = pd.DataFrame(outcomes)\n",
    "df_outcomes.to_csv(os.path.join(folder_name, f'scenario_space_100000_outcomes.csv'), index=False)\n",
    "\n",
    "print(f\"Experiments and outcomes saved in the '{folder_name}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d756e",
   "metadata": {},
   "source": [
    "### Subspace Partitioning\n",
    "\n",
    "This section generates data for the 'Subspace Partitioning' subsection. The goal is to partition the scenario space into smaller subspaces using Latin Hypercube Sampling (LHS). This allows for a more detailed exploration of the scenario and policy spaces.\n",
    "\n",
    "### Workflow:\n",
    "1. Define the number of scenarios (`n_scenarios = 600`) and policies (`n_policies = 400`).\n",
    "2. Use LHS sampling for both uncertainties and levers.\n",
    "3. Perform experiments using the `MultiprocessingEvaluator`.\n",
    "4. Save the generated experiments and outcomes to CSV files for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65318f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = 600\n",
    "n_policies = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model to get the LHS data for the subspace partitioning\n",
    "np.random.seed(21)\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios, policies=n_policies, uncertainty_sampling=Samplers.LHS, lever_sampling=Samplers.LHS)\n",
    "experiments, outcomes = results\n",
    "save_experiments_and_outcomes('LHS_using_pf2', experiments, outcomes, folder_name=\"LHS_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a1574",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis (Sobol Indices)\n",
    "\n",
    "This section generates data for the 'Sobol Indices' subsection. Sobol indices are used to perform sensitivity analysis, which helps identify the most influential uncertainties and levers in the model.\n",
    "\n",
    "### Workflow:\n",
    "1. Define a zero policy and a reference scenario with average values for uncertainties.\n",
    "2. Perform experiments using Sobol sampling for both scenarios and policies.\n",
    "3. Save the generated experiments and outcomes to CSV files for further analysis.\n",
    "\n",
    "### Notes:\n",
    "- The number of scenarios and policies must be a power of two for Sobol sampling.\n",
    "- The sensitivity analysis helps prioritize uncertainties and levers for optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ff2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all levers set to 0\n",
    "def get_do_nothing_dict():\n",
    "    return {l.name: 0 for l in dike_model.levers}\n",
    "\n",
    "# Create a policy using the do-nothing dictionary\n",
    "policies = [\n",
    "    Policy(\n",
    "        \"policy 0\",\n",
    "        **dict(\n",
    "            get_do_nothing_dict()\n",
    "        )\n",
    "    ),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe is used to show the average values of the uncertainty parameters\n",
    "df = pd.read_csv(\"zero_policy_results/scenario_space_100000_experiments.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scenario with average values for the uncertainties\n",
    "reference_values = {\n",
    "    \"Bmax\": 190,\n",
    "    \"Brate\": 4.167,\n",
    "    \"pfail\": 0.5,\n",
    "    \"discount rate 0\": 3,\n",
    "    \"discount rate 1\": 3,\n",
    "    \"discount rate 2\": 3,\n",
    "    \"ID flood wave shape\": 4, # Arbritary value, there is no average value for this parameter\n",
    "}\n",
    "\n",
    "# Create a scenario dictionary and then a Scenario object\n",
    "scenario_dict = {}\n",
    "for key in dike_model.uncertainties:\n",
    "    name_split = key.name.split(\"_\")\n",
    "    if len(name_split) == 1:\n",
    "        scenario_dict[key.name] = reference_values.get(key.name)\n",
    "    else:\n",
    "        scenario_dict[key.name] = reference_values.get(name_split[1])\n",
    "scenario = Scenario(\"default\", **scenario_dict)\n",
    "scenarios = [scenario,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b75096",
   "metadata": {},
   "source": [
    "Below the experiments can be run, the number of scenarios and policies chosen is very high, as this is needed to perform a sensitivity analysis using Sobol Indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of scenarios and policies must be a power of two for the Sobol' sequence\n",
    "n_scenarios = 4096\n",
    "n_policies = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model to get the sobol indices for the scenario space\n",
    "np.random.seed(21)\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios, policies=policies, uncertainty_sampling=Samplers.SOBOL)\n",
    "experiments, outcomes = results\n",
    "# Save the experiments and outcomes to CSV files\n",
    "save_experiments_and_outcomes('SOBOL_scenarios', experiments, outcomes, folder_name=\"sobol_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70543f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model to get the sobol indices for the policy space\n",
    "np.random.seed(21)\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=scenarios, policies=n_policies, lever_sampling=Samplers.SOBOL)   \n",
    "experiments, outcomes = results\n",
    "# Save the experiments and outcomes to CSV files\n",
    "save_experiments_and_outcomes('SOBOL_policies', experiments, outcomes, folder_name=\"sobol_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e8324af8a24",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Selecting Scenarios for Optimization\n",
    "\n",
    "This section focuses on selecting scenarios for optimization based on predefined bounds. The selected scenarios represent favorable, medium, and unfavorable conditions.\n",
    "\n",
    "### Workflow:\n",
    "1. Load the data generated in the 'Zero Policy' section.\n",
    "2. Define bounds for favorable, medium, and unfavorable scenarios.\n",
    "3. Filter and sample scenarios based on the defined bounds.\n",
    "4. Save the filtered scenarios to CSV files for use in optimization.\n",
    "\n",
    "### Notes:\n",
    "- The bounds are defined based on key metrics like expected annual damage and expected number of deaths.\n",
    "- A hardcoded seed (`42`) was used for sampling, which caused the sampled scenarios to be identical across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71229a6b5292c5ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:00:07.989542Z",
     "start_time": "2025-06-19T23:00:07.163799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the experiments DataFrame\n",
    "experiments = pd.read_csv(f\"zero_policy_results/scenario_space_100000_experiments.csv\")\n",
    "# Load the outcomes DataFrame\n",
    "outcomes = pd.read_csv(f\"zero_policy_results/scenario_space_100000_outcomes.csv\")\n",
    "combined_df = pd.concat([experiments, outcomes], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40f182e2e3a4dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:00:09.838150Z",
     "start_time": "2025-06-19T23:00:09.745121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define category-specific bounds\n",
    "favourable_bounds = [\n",
    "    [[0, 1], [0, 1]]  # 2 scenarios\n",
    "]\n",
    "\n",
    "unfavourable_bounds = [\n",
    "    [[6, float('inf')], [5, float('inf')]]  # 4 scenarios\n",
    "]\n",
    "\n",
    "medium_bounds = [\n",
    "    [[2.5, 3.5], [1.8, 2.8]],  # 2 scenarios\n",
    "    [[1.5, 2.5], [3, 4]]       # 2 scenarios\n",
    "]\n",
    "\n",
    "# Function to filter and sample based on bounds and desired number of samples\n",
    "def filter_and_sample(bounds_list, combined_df, n_per_bound):\n",
    "    results = pd.DataFrame()\n",
    "    for bound in bounds_list:\n",
    "        mask = (\n",
    "            (combined_df['Expected Annual Damage'] >= bound[0][0] * 1e9) &\n",
    "            (combined_df['Expected Annual Damage'] <= bound[0][1] * 1e9 if not np.isinf(bound[0][1]) else True) &\n",
    "            (combined_df['Expected Number of Deaths'] >= bound[1][0]) &\n",
    "            (combined_df['Expected Number of Deaths'] <= bound[1][1] if not np.isinf(bound[1][1]) else True)\n",
    "        )\n",
    "        filtered = combined_df[mask].copy()\n",
    "        if not filtered.empty:\n",
    "            sampled = filtered.sample(n=min(n_per_bound, len(filtered)), random_state=42) # Seed was predefined here :(\n",
    "            results = pd.concat([results, sampled], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No outcomes found for bounds {bound}.\")\n",
    "    return results\n",
    "\n",
    "# Apply to each category\n",
    "favourable_scenarios_df = filter_and_sample(favourable_bounds, combined_df, 2).iloc[:, :-39]\n",
    "medium_scenarios_df = filter_and_sample(medium_bounds, combined_df, 2).iloc[:, :-39]\n",
    "unfavourable_scenarios_df = filter_and_sample(unfavourable_bounds, combined_df, 4).iloc[:, :-39]\n",
    "\n",
    "# Save to CSV\n",
    "favourable_scenarios_df.to_csv(\"scenarios/favourable_scenarios_df.csv\", index=False)\n",
    "unfavourable_scenarios_df.to_csv(\"scenarios/unfavourable_scenarios_df.csv\", index=False)\n",
    "medium_scenarios_df.to_csv(\"scenarios/medium_scenarios_df.csv\", index=False)\n",
    "\n",
    "combined_df = pd.concat([favourable_scenarios_df, medium_scenarios_df, unfavourable_scenarios_df], ignore_index=True)\n",
    "combined_df.to_csv(\"scenarios/filtered_scenario_space_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e19ea656f1c21",
   "metadata": {},
   "source": [
    "\n",
    "## Running Optimization\n",
    "\n",
    "This section runs the optimization process for the selected scenarios. The optimization algorithm minimizes multiple objectives, such as expected annual damage and investment costs.\n",
    "\n",
    "### Workflow:\n",
    "1. Define the number of function evaluations (`nfe`) and epsilon values for the optimization algorithm.\n",
    "2. Run the optimization for each scenario using the `EpsNSGAII` algorithm.\n",
    "3. Save the results (outcomes and convergence metrics) to CSV files for further analysis.\n",
    "\n",
    "### Notes:\n",
    "- The optimization process uses the `MultiprocessingEvaluator` for parallel execution.\n",
    "- The results are used to create Pareto fronts, which represent the trade-offs between objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf18d294824a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(nfe, epsilon, seed, scenario=None):\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "    model, steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "    #Use default scenario if none is provided\n",
    "    if scenario is None:\n",
    "        reference_values = {\n",
    "            \"Bmax\": 175,\n",
    "            \"Brate\": 1.5,\n",
    "            \"pfail\": 0.5,\n",
    "            \"discount rate 0\": 3.5,\n",
    "            \"discount rate 1\": 3.5,\n",
    "            \"discount rate 2\": 3.5,\n",
    "            \"ID flood wave shape\": 4,\n",
    "        }\n",
    "        scenario_dict = {}\n",
    "        for key in model.uncertainties:\n",
    "            name_split = key.name.split(\"_\")\n",
    "            if len(name_split) == 1:\n",
    "                scenario_dict[key.name] = reference_values.get(key.name)\n",
    "            else:\n",
    "                scenario_dict[key.name] = reference_values.get(name_split[1])\n",
    "        scenario = Scenario(\"default\", **scenario_dict)\n",
    "\n",
    "    epsilons = [epsilon] * len(model.outcomes)\n",
    "    convergence_metrics = [EpsilonProgress()]\n",
    "\n",
    "\n",
    "    with MultiprocessingEvaluator(model) as evaluator:\n",
    "        results, convergence = evaluator.optimize(\n",
    "            nfe=nfe,\n",
    "            searchover=\"levers\",\n",
    "            algorithm=EpsNSGAII,\n",
    "            algorithm_kwargs={\n",
    "                \"epsilons\": epsilons,\n",
    "                \"problem\": scenario,\n",
    "                \"seed\": seed, # Seed run 1 = 20, seed run 2 = 21\n",
    "            },\n",
    "            convergence=convergence_metrics,\n",
    "            reference=scenario,\n",
    "            epsilons=epsilons,\n",
    "    )\n",
    "\n",
    "    return results, convergence\n",
    "\n",
    "def save_results_to_csv(result, name, seed):\n",
    "\n",
    "    # Define the folder path\n",
    "    folder_name = f\"optimization_150000_seed{seed}_results\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Save outcomes DataFrame to a CSV file in the folder\n",
    "    df_result = pd.DataFrame(result)\n",
    "    df_result.to_csv(os.path.join(folder_name, f'{name}_{current_date}.csv'), index=False)\n",
    "\n",
    "    print(f\"Saved {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d5ae28f4e92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenarios(unfavourable_scenarios_df, medium_scenarios_df, favourable_scenarios_df, seed, epsilon_value, nfe_value):\n",
    "    num = 0\n",
    "\n",
    "    for items in unfavourable_scenarios_df.iterrows():\n",
    "        print(f\"Running unfavourable scenario {num}...\")\n",
    "        scenario_dict = unfavourable_scenarios_df.iloc[num].to_dict()\n",
    "        scenario = Scenario(f\"unfavourable{num}\", **scenario_dict)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        result, convergence = run_optimization(epsilon=epsilon_value, nfe=nfe_value, seed=seed,scenario=scenario)\n",
    "\n",
    "        save_results_to_csv(result, f\"unfavourable_outcomes_{nfe_value}_run{seed}_{num}\", seed)\n",
    "        save_results_to_csv(convergence, f\"unfavourable_convergence_{nfe_value}_run{seed}_{num}\", seed)\n",
    "        num += 1\n",
    "\n",
    "    num = 0\n",
    "    for items in medium_scenarios_df.iterrows():\n",
    "        print(f\"Running medium scenario {num}...\")\n",
    "        scenario_dict = medium_scenarios_df.iloc[num].to_dict()\n",
    "        scenario = Scenario(f\"medium{num}\", **scenario_dict)\n",
    "        result, convergence = run_optimization(epsilon=epsilon_value, nfe=nfe_value, seed=seed, scenario=scenario)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        save_results_to_csv(result, f\"medium_outcomes_{nfe_value}_run{seed}_{num}\", seed)\n",
    "        save_results_to_csv(convergence, f\"medium_convergence_{nfe_value}_run{seed}_{num}\", seed)\n",
    "        num += 1\n",
    "\n",
    "    num = 0\n",
    "    for items in favourable_scenarios_df.iterrows():\n",
    "        print(f\"Running favourable scenario {num}...\")\n",
    "        scenario_dict = favourable_scenarios_df.iloc[num].to_dict()\n",
    "        scenario = Scenario(f\"favourable{num}\", **scenario_dict)\n",
    "        result, convergence = run_optimization(epsilon=epsilon_value, nfe=nfe_value, scenario=scenario)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        save_results_to_csv(result, f\"favourable_outcomes_150000_run{seed}_{num}\", seed)\n",
    "        save_results_to_csv(convergence, f\"favourable_convergence_150000_run{seed}_{num}\", seed)\n",
    "        num += 1\n",
    "\n",
    "run_scenarios(unfavourable_scenarios_df, medium_scenarios_df, favourable_scenarios_df, 20, 1000, 150000)\n",
    "run_scenarios(unfavourable_scenarios_df, medium_scenarios_df, favourable_scenarios_df, 21, 1000, 150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d80272a92bd12f",
   "metadata": {},
   "source": [
    "## Selecting Policies from Pareto Fronts\n",
    "\n",
    "This section selects the best policies from the Pareto fronts generated in the optimization process. The selection is based on thresholds and weighted scores.\n",
    "\n",
    "### Workflow:\n",
    "1. Define thresholds for key metrics like investment costs and expected annual damage.\n",
    "2. Filter policies based on the defined thresholds.\n",
    "3. Select the top policies by minimizing a weighted score that combines multiple robustness metrics.\n",
    "4. Save the selected policies to CSV files for further analysis.\n",
    "\n",
    "### Notes:\n",
    "- The weighted score reflects the importance of different robustness metrics for each outcome.\n",
    "- The selected policies are tested for robustness in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c20240539d53cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:04:45.535711Z",
     "start_time": "2025-06-19T23:04:45.451565Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_scenario_folder(folder):\n",
    "    \"\"\"\n",
    "    Loads and combines CSV files from a folder by scenario type ('favourable', 'medium', 'unfavourable').\n",
    "    Adds 'Total Investment Costs' and 'Expected Annual Costs' columns, and labels each row with a scenario ID.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing scenario CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - favourable_df, unfavourable_df, medium_df (DataFrames): Combined data for each scenario type.\n",
    "    \"\"\"\n",
    "    # === Step 1: List and categorize CSV files ===\n",
    "    files = os.listdir(folder)\n",
    "    favourable_files = [f for f in files if f.startswith(\"favourable_outcomes\")]\n",
    "    unfavourable_files = [f for f in files if f.startswith(\"unfavourable_outcomes\")]\n",
    "    medium_files = [f for f in files if f.startswith(\"medium_outcomes\")]\n",
    "\n",
    "    # === Step 2: Define helper to load and combine CSVs ===\n",
    "    def load_and_combine(file_list, label):\n",
    "        dfs = []\n",
    "        for run, file in enumerate(file_list):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df['Total Investment Costs'] = (\n",
    "                df['Dike Investment Costs'] +\n",
    "                df['RfR Investment Costs']\n",
    "            )\n",
    "            df['Expected Annual Costs'] = (\n",
    "                df['Evacuation Costs'] +\n",
    "                df['Expected Annual Damage']\n",
    "            )\n",
    "            df['Scenario'] = f\"{label}_{run}\"\n",
    "            dfs.append(df)\n",
    "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "    # === Step 3: Load and combine each group ===\n",
    "    favourable_df = load_and_combine(favourable_files, 'Favourable')\n",
    "    unfavourable_df = load_and_combine(unfavourable_files, 'Unfavourable')\n",
    "    medium_df = load_and_combine(medium_files, 'Medium')\n",
    "\n",
    "    return favourable_df, unfavourable_df, medium_df\n",
    "\n",
    "folder_path = \"optimization_150000_seed20_results\"\n",
    "favourable_df_20, unfavourable_df_20, medium_df_20 = process_scenario_folder(folder_path)\n",
    "\n",
    "folder_path = \"optimization_150000_seed21_results\"\n",
    "favourable_df_21, unfavourable_df_21, medium_df_21 = process_scenario_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551d64eaf431e63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:06:30.095815Z",
     "start_time": "2025-06-19T23:06:30.003951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants for sort priority\n",
    "SORT_COLUMNS = [\n",
    "    'Expected Number of Deaths',\n",
    "    'Expected Annual Damage',\n",
    "    'Evacuation Costs',\n",
    "    'Total Investment Costs'\n",
    "]\n",
    "\n",
    "# Threshold quantile defaults\n",
    "DEFAULT_THRESHOLDS = {\n",
    "    'q_investment_costs': 0.75,\n",
    "    'q_dike': 0.75,\n",
    "    'q_evac': 0.50,\n",
    "    'q_damage': 0.25,\n",
    "    'death_cap': 0.02\n",
    "}\n",
    "\n",
    "def load_thresholds(df, thresholds=DEFAULT_THRESHOLDS):\n",
    "    df = df.copy()\n",
    "    df['Total Investment Costs'] = df['Dike Investment Costs'] + df['RfR Investment Costs']\n",
    "    df['Expected Annual Costs'] = df['Evacuation Costs'] + df['Expected Annual Damage']\n",
    "\n",
    "    threshold_values = {\n",
    "        'Total Investment Costs': df['Total Investment Costs'].quantile(thresholds['q_investment_costs']),\n",
    "        'Dike Investment Costs': df['Dike Investment Costs'].quantile(thresholds['q_dike']),\n",
    "        'Evacuation Costs': df['Evacuation Costs'].quantile(thresholds['q_evac']),\n",
    "        'Expected Annual Damage': df['Expected Annual Damage'].quantile(thresholds['q_damage']),\n",
    "        'Expected Number of Deaths': thresholds['death_cap']\n",
    "    }\n",
    "\n",
    "    return df, threshold_values\n",
    "\n",
    "def filter_df(df, threshold_values):\n",
    "    return df[\n",
    "        (df['Total Investment Costs'] <= threshold_values['Total Investment Costs']) &\n",
    "        (df['Dike Investment Costs'] <= threshold_values['Dike Investment Costs']) &\n",
    "        (df['Evacuation Costs'] <= threshold_values['Evacuation Costs']) &\n",
    "        (df['Expected Annual Damage'] <= threshold_values['Expected Annual Damage']) &\n",
    "        (df['Expected Number of Deaths'] <= threshold_values['Expected Number of Deaths']) &\n",
    "        (df['RfR Investment Costs'] > 0)\n",
    "    ]\n",
    "\n",
    "def select_top_policies(filtered_df, top_n=3):\n",
    "    return filtered_df.sort_values(by=SORT_COLUMNS).head(top_n)\n",
    "\n",
    "def save_policy_csv(results_df, prefix, folder_name, save=True):\n",
    "    if save:\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        filename = os.path.join(folder_name, f\"{prefix}policy.csv\")\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "def process_policy_sets(fav_df, med_df, unfav_df, seed):\n",
    "    fav_loaded, fav_thresholds = load_thresholds(fav_df)\n",
    "    med_loaded, med_thresholds = load_thresholds(med_df)\n",
    "    unfav_loaded, unfav_thresholds = load_thresholds(unfav_df)\n",
    "\n",
    "    fav_selected = filter_df(fav_loaded, fav_thresholds)\n",
    "    med_selected = filter_df(med_loaded, med_thresholds)\n",
    "    unfav_selected = filter_df(unfav_loaded, unfav_thresholds)\n",
    "\n",
    "    # Save filtered dataframes\n",
    "    save_policy_csv(fav_selected, f\"favourable_{seed}_\", folder_name=f\"policies_seed_{seed}\")\n",
    "    save_policy_csv(med_selected, f\"medium_{seed}_\", folder_name=f\"policies_seed_{seed}\")\n",
    "    save_policy_csv(unfav_selected, f\"unfavourable_{seed}_\", folder_name=f\"policies_seed_{seed}\")\n",
    "\n",
    "    # Top 3 selection from each category\n",
    "    top3_combined = pd.concat([\n",
    "        select_top_policies(fav_selected),\n",
    "        select_top_policies(med_selected),\n",
    "        select_top_policies(unfav_selected)\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # Save combined top policies\n",
    "    save_policy_csv(top3_combined, f\"top3_combined_{seed}_\", folder_name=f\"policies_seed_{seed}\")\n",
    "\n",
    "    return top3_combined\n",
    "\n",
    "# Call the function for both seeds/years\n",
    "combined_top_policies_20 = process_policy_sets(\n",
    "    favourable_df_20, medium_df_20, unfavourable_df_20, seed=\"20\"\n",
    ")\n",
    "\n",
    "combined_top_policies_21 = process_policy_sets(\n",
    "    favourable_df_21, medium_df_21, unfavourable_df_21, seed=\"21\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478046b4679ba27",
   "metadata": {},
   "source": [
    "## Evaluating Robustness\n",
    "\n",
    "This section evaluates the robustness of the selected policies against the set of 10 scenarios. Robustness metrics include mean, standard deviation, mean regret, and max regret.\n",
    "\n",
    "### Workflow:\n",
    "1. Load the results of the selected policies.\n",
    "2. Calculate robustness metrics for each policy.\n",
    "3. Normalize the metrics to a [0,1] scale.\n",
    "4. Apply weights to the normalized metrics to calculate a composite score.\n",
    "5. Select the top policies based on the composite score.\n",
    "\n",
    "### Notes:\n",
    "- The robustness evaluation helps identify policies that perform well across a wide range of scenarios.\n",
    "- The weights reflect the relative importance of different robustness metrics for each outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790ec8e9b0e50e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:08:01.468264Z",
     "start_time": "2025-06-19T23:07:47.098600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the dike model and planning steps with formulation version 2\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "# Load the filtered scenario space CSV file containing scenario parameters\n",
    "scenario_collection_df = pd.read_csv(\"scenarios/filtered_scenario_space_df.csv\")\n",
    "\n",
    "# Ignore the last 9 columns which may be metrics or results, keeping only policy parameters\n",
    "\n",
    "combined_top_policies_df_20 = combined_top_policies_20.iloc[:, :-8]\n",
    "combined_top_policies_df_21 = combined_top_policies_21.iloc[:, :-8]\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "def run_policies_in_scenarios(dike_model, policy_df, scenario_df, seed):\n",
    "    # Create a timestamped output folder to store results\n",
    "    output_folder = f\"policy_scenario_results_{seed}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Loop over each policy in the DataFrame\n",
    "    for i, (idx, policy_row) in enumerate(policy_df.iterrows()):\n",
    "        print(f\"Running policy {i}...\")\n",
    "\n",
    "        # Create an EMA Workbench Policy object from the current policy row\n",
    "        policy = Policy(f\"policy_{i}\", **policy_row.to_dict())\n",
    "\n",
    "        # Create Scenario objects for all scenarios in scenario_df\n",
    "        scenarios = []\n",
    "        for j, scenario_row in scenario_df.iterrows():\n",
    "            scenario = Scenario(f\"scenario_{j}\", **scenario_row.to_dict())\n",
    "            scenarios.append(scenario)\n",
    "\n",
    "        # Use a multiprocessing evaluator to run experiments for the given policy across all scenarios\n",
    "        with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "            experiments, outcomes = evaluator.perform_experiments(\n",
    "                scenarios=scenarios,\n",
    "                policies=[policy]\n",
    "            )\n",
    "\n",
    "        # Convert experiments (input params) to a DataFrame\n",
    "        results_df = pd.DataFrame(experiments)\n",
    "\n",
    "        # Add each outcome as a new column in the results DataFrame\n",
    "        for outcome_name, outcome_values in outcomes.items():\n",
    "            results_df[outcome_name] = outcome_values\n",
    "\n",
    "        # Save results to a CSV file inside the output folder\n",
    "        filename = os.path.join(output_folder, f\"policy_{i}_results.csv\")\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved results for policy {i} to {filename}\")\n",
    "\n",
    "# Initialize model and planning steps with formulation version 2\n",
    "model, steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "# Run the policies on scenarios\n",
    "run_policies_in_scenarios(model, combined_top_policies_df_20, scenario_collection_df, 20)\n",
    "run_policies_in_scenarios(model, combined_top_policies_df_21, scenario_collection_df, 21)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295963d01393a0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:35:22.010766Z",
     "start_time": "2025-06-19T23:35:21.876425Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_robustness_with_regret(df, policy_id_col, scenario_id_col, performance_cols, weights):\n",
    "    # Copy dataframe to avoid modifying original\n",
    "    regret_df = df.copy()\n",
    "\n",
    "    # Calculate regret per scenario and performance metric:\n",
    "    # Regret = (value for policy) - (best value among all policies in that scenario)\n",
    "    # This tells how much worse a policy performs compared to the best policy in each scenario\n",
    "    for col in performance_cols:\n",
    "        regret_df[f'{col} Regret'] = regret_df.groupby(scenario_id_col)[col].transform(lambda x: x - x.min())\n",
    "\n",
    "    # Group data by policy to compute summary statistics per policy\n",
    "    grouped = regret_df.groupby(policy_id_col)\n",
    "    results = []\n",
    "\n",
    "    for policy, group in grouped:\n",
    "        metrics = {'Policy': policy}\n",
    "\n",
    "        # For each performance metric, calculate raw statistics:\n",
    "        # Mean: average performance over all scenarios\n",
    "        # Std: variability in performance\n",
    "        # Max Regret: worst-case regret (how badly it can perform compared to best scenario)\n",
    "        # Mean Regret: average regret over all scenarios (typical performance gap)\n",
    "        for col in performance_cols:\n",
    "            values = group[col]\n",
    "            regrets = group[f'{col} Regret']\n",
    "\n",
    "            metrics[f'{col} Mean'] = values.mean()\n",
    "            metrics[f'{col} Std'] = values.std()\n",
    "            metrics[f'{col} Max Regret'] = regrets.max()\n",
    "            metrics[f'{col} Mean Regret'] = regrets.mean()\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    robustness_df = pd.DataFrame(results)\n",
    "\n",
    "    # --- Normalization Section ---\n",
    "    # We want to combine metrics that have different scales into a single score.\n",
    "    # Normalization rescales each metric to a [0,1] range where 0 = best (lowest value),\n",
    "    # 1 = worst (highest value), so metrics are comparable.\n",
    "    for col in performance_cols:\n",
    "        for stat in ['Mean', 'Std', 'Max Regret', 'Mean Regret']:\n",
    "            metric_col = f'{col} {stat}'\n",
    "\n",
    "            # Find min and max values of this metric across all policies\n",
    "            min_val = robustness_df[metric_col].min()\n",
    "            max_val = robustness_df[metric_col].max()\n",
    "\n",
    "            # Denominator avoids division by zero if min == max (all values equal)\n",
    "            denom = max_val - min_val if max_val > min_val else 1e-9\n",
    "\n",
    "            # Normalize: subtract min then divide by range => scales to 0-1\n",
    "            robustness_df[f'{metric_col} (Norm)'] = (robustness_df[metric_col] - min_val) / denom\n",
    "\n",
    "    # --- Weighted Scoring Section ---\n",
    "    # Apply user-defined weights to normalized metrics reflecting their importance.\n",
    "    # Weights correspond to your criteria priority for mean, std, max regret, mean regret.\n",
    "    # Weighted sum gives a single score to rank policies.\n",
    "    score_cols = []\n",
    "    for col in performance_cols:\n",
    "        for stat, weight in zip(['Mean', 'Std', 'Max Regret', 'Mean Regret'], weights[col]):\n",
    "            norm_col = f'{col} {stat} (Norm)'\n",
    "            weighted_col = f'{col} {stat} Weighted'\n",
    "            robustness_df[weighted_col] = robustness_df[norm_col] * weight\n",
    "            score_cols.append(weighted_col)\n",
    "\n",
    "    # Sum all weighted metrics to get a final composite score\n",
    "    # Lower score indicates better overall performance based on priorities\n",
    "    robustness_df['Total Score'] = robustness_df[score_cols].sum(axis=1)\n",
    "\n",
    "    return robustness_df\n",
    "\n",
    "def find_best_policies(folder_path):\n",
    "    all_dfs = []\n",
    "\n",
    "    # Load all policy result CSV files from the folder\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\"_results.csv\"):\n",
    "            policy_id = file.replace(\"_results.csv\", \"\")\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            df['policy'] = policy_id\n",
    "            df['scenario'] = df.index\n",
    "\n",
    "            # Combine investment costs from different sources\n",
    "            df['Total Investment Costs'] = df['Dike Investment Costs'] + df['RfR Investment Costs']\n",
    "            df['Expected Annual Costs'] = df['Evacuation Costs'] + df['Expected Annual Damage']\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No result CSV files found.\")\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    performance_columns = [\n",
    "        'Expected Number of Deaths',\n",
    "        'Expected Annual Damage',\n",
    "        'Evacuation Costs',\n",
    "        'Total Investment Costs'\n",
    "    ]\n",
    "\n",
    "    # Weights according to your priority criteria:\n",
    "    # - Mean: importance decreases from deaths to investment costs\n",
    "    # - Std: used for damage, evacuation, investment\n",
    "    # - Max Regret: only for deaths\n",
    "    # - Mean Regret: damage, evacuation, investment in descending importance\n",
    "    weights = {\n",
    "        'Expected Number of Deaths': [1.0, 1.0, 1.0, 0.01],   # Mean, Std, Max Regret, Mean Regret\n",
    "        'Expected Annual Damage':   [0.75, 0.75, 0.01, 0.5],\n",
    "        'Evacuation Costs':         [0.5, 0.5, 0.01, 0.25],\n",
    "        'Total Investment Costs':   [0.25, 0.25, 0.01, 0.01],\n",
    "    }\n",
    "\n",
    "    robustness_df = evaluate_robustness_with_regret(\n",
    "        combined_df,\n",
    "        policy_id_col='policy',\n",
    "        scenario_id_col='scenario',\n",
    "        performance_cols=performance_columns,\n",
    "        weights=weights\n",
    "    )\n",
    "\n",
    "    # Sort policies by total score ascending (lower is better)\n",
    "    sorted_df = robustness_df.sort_values('Total Score').reset_index(drop=True)\n",
    "\n",
    "    # Select top 2 policies based on the weighted score\n",
    "    best_two = sorted_df.head(2)\n",
    "\n",
    "    # Print raw performance metrics for the top 2 policies\n",
    "        # Print raw performance metrics for the top 2 policies\n",
    "    # Print raw performance metrics for the top 2 policies in a compact tabular format\n",
    "    for i, row in best_two.iterrows():\n",
    "        print(f\"\\nRank {i+1} Policy: {row['Policy']}\")\n",
    "        print(f\"{'Metric':30} {'Mean':>15} {'Std':>15} {'Max Regret':>15} {'Mean Regret':>15}\")\n",
    "        print(\"-\" * 90)\n",
    "        for col in performance_columns:\n",
    "            print(f\"{col:30} \"\n",
    "                  f\"{row[f'{col} Mean']:15.6g} \"\n",
    "                  f\"{row[f'{col} Std']:15.6g} \"\n",
    "                  f\"{row[f'{col} Max Regret']:15.6g} \"\n",
    "                  f\"{row[f'{col} Mean Regret']:15.6g}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Return the IDs and full dataframes of the two best policies for further analysis\n",
    "    best_policy_ids = best_two['Policy'].tolist()\n",
    "    dfs = [combined_df[combined_df['policy'] == pid] for pid in best_policy_ids]\n",
    "\n",
    "    return best_policy_ids, dfs\n",
    "\n",
    "\n",
    "output_folder = \"policy_scenario_results_20\"\n",
    "best_policy_ids_20, best_policy_dfs_20 = find_best_policies(output_folder)\n",
    "\n",
    "output_folder = \"policy_scenario_results_21\"\n",
    "best_policy_ids_21, best_policy_dfs_21 = find_best_policies(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed0a05223daa4b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:12:08.825508Z",
     "start_time": "2025-06-19T23:12:08.801196Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = 'selected_4_policies'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save policy_8 s20\n",
    "df_policy_8 = pd.DataFrame(best_policy_dfs_20[0])\n",
    "file_name_8 = f'policy_8_overview_20.csv'\n",
    "df_policy_8.to_csv(os.path.join(output_folder, file_name_8), index=False)\n",
    "\n",
    "# Save policy_6 s20\n",
    "df_policy_6 = pd.DataFrame(best_policy_dfs_20[1])\n",
    "file_name_6 = f'policy_6_overview_20.csv'\n",
    "df_policy_6.to_csv(os.path.join(output_folder, file_name_6), index=False)\n",
    "\n",
    "# Save policy_8 s21\n",
    "df_policy_8 = pd.DataFrame(best_policy_dfs_21[0])\n",
    "file_name_8 = f'policy_8_overview_21.csv'\n",
    "df_policy_8.to_csv(os.path.join(output_folder, file_name_8), index=False)\n",
    "\n",
    "# Save policy_7 s21\n",
    "df_policy_7 = pd.DataFrame(best_policy_dfs_21[1])\n",
    "file_name_7 = f'policy_7_overview_21.csv'\n",
    "df_policy_7.to_csv(os.path.join(output_folder, file_name_7), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694bc7fe66ef8ae",
   "metadata": {},
   "source": [
    "## Final Robustness Testing\n",
    "\n",
    "This section tests the robustness of the final selected policies against 100,000 new scenarios. The goal is to ensure that the policies perform well under diverse conditions.\n",
    "\n",
    "### Workflow:\n",
    "1. Load the selected policies and new scenarios.\n",
    "2. Run experiments for each policy across the new scenarios.\n",
    "3. Save the results to CSV files for further analysis.\n",
    "\n",
    "### Notes:\n",
    "- The robustness testing provides a final validation of the selected policies.\n",
    "- The results are used to calculate robustness metrics and identify the best-performing policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2098006273ac3d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T22:25:04.741536Z",
     "start_time": "2025-06-19T22:25:04.708223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the folder path and get list of files\n",
    "folder_name = \"selected_4_policies\"\n",
    "file_list = os.listdir(folder_name)\n",
    "\n",
    "# Initialize empty lists to hold dataframes for policies_20 and policies_21\n",
    "policies_20_dfs = []\n",
    "policies_21_dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(os.path.join(folder_name, file), nrows=1)\n",
    "\n",
    "        # Check if file name contains \"_20_\" or \"_21_\" and add sliced df accordingly\n",
    "        if '_20_' in file:\n",
    "            # Take rows 20 to 50 by index (i.e. iloc[19:50])\n",
    "            policies_20_dfs.append(df)\n",
    "        elif '_21_' in file:\n",
    "            policies_21_dfs.append(df)\n",
    "\n",
    "\n",
    "policies_20_df = pd.concat(policies_20_dfs, ignore_index=True).iloc[:, 19:50]\n",
    "policies_21_df = pd.concat(policies_21_dfs, ignore_index=True).iloc[:, 19:50]\n",
    "\n",
    "policies_20 = []\n",
    "\n",
    "for idx, row in policies_20_df.iterrows():\n",
    "    # Convert the row (pandas Series) to a dictionary: lever_name -> lever_value\n",
    "    lever_values = row.to_dict()\n",
    "\n",
    "    # Create a unique policy name, e.g. \"policy_20_row_0\"\n",
    "    policy_name = f\"policy_20_row_{idx}\"\n",
    "\n",
    "    # Create the Policy object\n",
    "    policy = Policy(policy_name, **lever_values)\n",
    "\n",
    "    # Append to the list\n",
    "    policies_20.append(policy)\n",
    "\n",
    "policies_21 = []\n",
    "\n",
    "for idx, row in policies_21_df.iterrows():\n",
    "    # Convert the row (pandas Series) to a dictionary: lever_name -> lever_value\n",
    "    lever_values = row.to_dict()\n",
    "\n",
    "    # Create a unique policy name, e.g. \"policy_20_row_0\"\n",
    "    policy_name = f\"policy_21_row_{idx}\"\n",
    "\n",
    "    # Create the Policy object\n",
    "    policy = Policy(policy_name, **lever_values)\n",
    "\n",
    "    # Append to the list\n",
    "    policies_21.append(policy)\n",
    "\n",
    "def save_experiments_and_outcomes(name, experiments, outcomes, folder_name=\"robustness_results\"):\n",
    "    \"\"\"\n",
    "    Saves the experiments and outcomes to CSV files in the specified folder,\n",
    "    with filenames that include a custom name and timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - name (str): Custom label for the output files (e.g., 'policy6_seed20').\n",
    "    - experiments (pd.DataFrame): The experiments DataFrame.\n",
    "    - outcomes (dict): The outcomes dictionary from the EMA Workbench.\n",
    "    - folder_name (str): The name of the folder where files will be saved. Default is \"results\".\n",
    "    \"\"\"\n",
    "    # Get timestamp\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Define file paths\n",
    "    exp_path = os.path.join(folder_name, f'{name}_robustness_100000_experiments_{current_date}.csv')\n",
    "    out_path = os.path.join(folder_name, f'{name}_robustness_100000_outcomes_{current_date}.csv')\n",
    "\n",
    "    # Save experiments\n",
    "    experiments.to_csv(exp_path, index=False)\n",
    "\n",
    "    # Flatten outcomes and save\n",
    "    flattened_data = {key: value.flatten() for key, value in outcomes.items()}\n",
    "    df_outcomes = pd.DataFrame(flattened_data)\n",
    "    df_outcomes.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Saved experiments and outcomes as '{name}' in '{folder_name}' (timestamp: {current_date})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6a30962b59d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T22:26:04.921364300Z",
     "start_time": "2025-06-19T22:25:10.023683Z"
    }
   },
   "outputs": [],
   "source": [
    "n_scenarios = 100000\n",
    "np.random.seed(21)\n",
    "\n",
    "def run_policy(name, policy, n_scenarios):\n",
    "    # Load your model and define parameters\n",
    "    dike_model, planning_steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "    with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "        results = evaluator.perform_experiments(\n",
    "            scenarios=n_scenarios,\n",
    "            policies=policy,\n",
    "            uncertainty_sampling=Samplers.LHS\n",
    "        )\n",
    "    experiments, outcomes = results\n",
    "    save_experiments_and_outcomes(name, experiments, outcomes)\n",
    "\n",
    "run_policy('policy_6_s20', policies_20[0], n_scenarios)\n",
    "run_policy('policy_8_s20', policies_20[1], n_scenarios)\n",
    "run_policy('policy_7_s21', policies_21[0], n_scenarios)\n",
    "run_policy('policy_8_s21', policies_21[1], n_scenarios)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1709655197acc",
   "metadata": {},
   "source": [
    "### Generating Outcomes for Different Aggregation\n",
    "\n",
    "Policies are evaluated under a different aggregation level, limited to 50,000 scenarios due to time constraints. The `run_policy` function executes experiments using problem formulation 3, seed 21, performs sampling with LHS, and saves results for four specific policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e60f4717d5383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = 50000\n",
    "np.random.seed(21)\n",
    "\n",
    "def run_policy(name, policy, n_scenarios):\n",
    "    # Load your model and define parameters\n",
    "    dike_model, planning_steps = get_model_for_problem_formulation(3)\n",
    "\n",
    "    with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "        results = evaluator.perform_experiments(\n",
    "            scenarios=n_scenarios,\n",
    "            policies=policy,\n",
    "            uncertainty_sampling=Samplers.LHS\n",
    "        )\n",
    "    experiments, outcomes = results\n",
    "    save_experiments_and_outcomes(name, experiments, outcomes)\n",
    "\n",
    "run_policy('policy_6_s20', policies_20[0], n_scenarios)\n",
    "run_policy('policy_8_s20', policies_20[1], n_scenarios)\n",
    "run_policy('policy_7_s21', policies_21[0], n_scenarios)\n",
    "run_policy('policy_8_s21', policies_21[1], n_scenarios)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
